VocalCoach AI - Technical Roadmap v2.0
Status: Alpha -> Moving to Beta
Focus: Robustheit, Real-time Feedback & AI Memory
üö® Priority 1: Core Algorithm Refinement (Immediate Fixes)
Das Fundament muss stimmen, bevor wir Features bauen.
[ ] Smart Pitch Filtering (Fix Range Finder)
Datei: backend/analysis/pitch.py
Task: Ersetze np.min/max durch np.percentile(f0, 10) und np.percentile(f0, 90).
Ziel: Ignorieren von Ausrei√üern (Husten, Quietschen, Raumklang), damit ein Bariton nicht als Mezzo erkannt wird.
[ ] Voice Type Calibration
Datei: backend/main.py -> /analyze/range
Task: Implementiere "Bottom-Heavy Logic". Die tiefste Note ist physiologisch begrenzter als die h√∂chste (Falsett/Whistle). Gewichte min_pitch st√§rker bei der Fach-Erkennung.
üöÄ Priority 2: The "Live" Experience (Frontend Engineering)
Weg von "Black Box", hin zu visuellem Echtzeit-Feedback.
[ ] Real-time Pitch Detection (Client-Side)
Tech: Integration von ml5.js (PitchDetection) oder Aubio (via WebAssembly) direkt in React.
UI: Visualisierung einer "Pitch Line" √ºber einem Canvas w√§hrend der Aufnahme.
Nutzen: Der User sieht sofort, ob er den Ton h√§lt, nicht erst nach dem Upload.
[ ] Interactive Piano Roll
Komponente: ExerciseModal
Task: Statt nur Audio abzuspielen, visualisiere die Ziel-Noten als Balken (wie Guitar Hero).
Sync: Synchronisiere die Pitch-Linie des Users mit den Ziel-Balken.
üß† Priority 3: The "Deep" Coach (AI Memory)
Die KI soll sich erinnern, nicht nur reagieren.
[ ] History Injection
Backend: Erweitere generate_feedback in ai_wrapper.py.
Logic: Hole die letzten 5 Sessions des Users aus der DB.
Prompt: "Vergleiche die aktuelle Session mit dem Durchschnitt der letzten 5. Ist der Trend positiv oder negativ?"
[ ] Trend Analysis Endpoints
API: GET /stats/trends
Return: Array von [date, jitter_value, pitch_accuracy] f√ºr Chart.js Graphen im Dashboard.
üõ†Ô∏è Priority 4: Advanced Audio Features
[ ] MIDI Support
Backend: backend/analysis/alignment.py
Funktion: Dynamic Time Warping (DTW) um die gesungene Melodie zeitlich an eine Soll-Melodie (MIDI) anzupassen.
Ziel: Bewertung von Timing und Phrasierung bei echten Songs.
[ ] Formant Biofeedback
Visuell: Echtzeit-Anzeige des "S√§ngerformanten" (2-4kHz Energie).
Feedback: "Mach den Klang heller/dunkler", indem man sieht, wie sich das Spektrum √§ndert.
üì± Priority 5: Architecture Polish
[ ] Dockerization
Erstelle Dockerfile und docker-compose.yml f√ºr Backend (Python) und Frontend (Node), um das Deployment zu vereinfachen.
[ ] Async Processing
Verschiebe die parselmouth Analyse in einen Background Worker (Celery/Redis), falls die Analysen l√§nger als 2-3 Sekunden dauern.



VocalCoach AI - Technical Roadmap v3 (The Interactive Era)

Status: Core Analysis Stable ‚úÖ | Focus: Interactive UX & Advanced Algorithms üöß

üéØ Epic 1: The "Piano Roll" Experience (Frontend Heavy)

Ziel: Der User soll nicht nur eine Wellenform sehen, sondern seine Stimme als Linie auf Noten-Balken.

1.1 Canvas Piano Roll (Visualisierung)

Datei: src/components/AudioRecorder.jsx (oder neue PianoRoll.jsx)

Konzept:

Y-Achse: Logarithmische Frequenz (Noten C2 bis C6).

X-Achse: Zeit (Scrollend).

Layer 1 (Target): Graue Balken zeigen die Soll-Noten der √úbung (aus exercise.pattern).

Layer 2 (User): Eine leuchtende Linie (z.B. Cyan) zeigt den ml5 Pitch in Echtzeit.

Tech: HTML5 Canvas requestAnimationFrame.

1.2 Feedback-Loop (Visuell)

F√§rbe die User-Linie Gr√ºn, wenn sie den grauen Balken trifft, und Rot, wenn sie daneben liegt.

Das gibt sofortiges Dopamin beim √úben.

üß† Epic 2: Algorithmic Accuracy (Backend Heavy)

Ziel: Die "Trefferquote" bei Melodien (Scales) wissenschaftlich berechnen.

2.1 Dynamic Time Warping (DTW)

Problem: analyze_pitch_accuracy z√§hlt aktuell nur "Hits". Wenn der User zu langsam/schnell singt, versagt der Vergleich.

L√∂sung: Implementierung des DTW-Algorithmus (fastdtw oder librosa.sequence.dtw).

Logik:

Extrahiere Pitch-Kurve der Referenz (Synth).

Extrahiere Pitch-Kurve des Users.

DTW "dehnt/staucht" die Zeitachsen, um die bestm√∂gliche √úbereinstimmung zu finden.

Der "Distance Score" ist dein Ma√ü f√ºr Intonation & Phrasierung (unabh√§ngig vom Tempo).

2.2 Rhythm Analysis

Nutze librosa.onset.onset_detect, um zu pr√ºfen: War der User rhythmisch "tight" auf dem Beat des Playbacks?

üìä Epic 3: Long-Term Vocal Health (Data Science)

Ziel: Trends erkennen, die ein einzelner Tag nicht zeigt.

3.1 Dashboard Charts (recharts)

Visualisiere den Verlauf von:

Jitter: Wird die Stimme √ºber Wochen "klarer"?

Range: Hat sich der h√∂chste Ton nach oben verschoben?

Stamina: Wird die MPT (Atemdauer) l√§nger?

3.2 Der "Vocal Health Monitor"

Wenn Jitter 3 Sessions in Folge steigt -> Warnung: "Deine Stimme scheint erm√ºdet. Mach 2 Tage Pause." (KI Proaktivit√§t).

üõ†Ô∏è Technical Debt & Refactoring

[ ] Frontend State Management:

Mit steigender Komplexit√§t (Recorder, Modal, User, History) sollten wir Zustand (Zustand Library) oder Context API sauber aufsetzen, um "Prop Drilling" zu vermeiden.

[ ] Async Processing:

Die Analyse dauert ca. 1-2 Sekunden.

UI: Zeige coole Loading-Animationen (z.B. eine schwingende Stimmgabel), w√§hrend das Backend rechnet.
